{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d531447",
   "metadata": {},
   "source": [
    "## CIFAR 10\n",
    "\n",
    "classification (10 classes) \n",
    "\n",
    "CIFAR 10 consists of: 60,000 tiny 32 x 32 color RGB images\n",
    "\n",
    "labeled with integer 1 to 10 classes \n",
    "\n",
    "airplane (0), car (1), etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ec8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "## import imageio\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score, accuracy_score, f1_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39a0c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/cifar10data/'\n",
    "\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395a6942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c597c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b830b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data/cifar10data/\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "print(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbec4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = cifar10[79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "315b8b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c5ae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51f0e58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AugMix', 'AutoAugment', 'AutoAugmentPolicy', 'CenterCrop', 'ColorJitter', 'Compose', 'ConvertImageDtype', 'FiveCrop', 'GaussianBlur', 'Grayscale', 'InterpolationMode', 'Lambda', 'LinearTransformation', 'Normalize', 'PILToTensor', 'Pad', 'RandAugment', 'RandomAdjustSharpness', 'RandomAffine', 'RandomApply', 'RandomAutocontrast', 'RandomChoice', 'RandomCrop', 'RandomEqualize', 'RandomErasing', 'RandomGrayscale', 'RandomHorizontalFlip', 'RandomInvert', 'RandomOrder', 'RandomPerspective', 'RandomPosterize', 'RandomResizedCrop', 'RandomRotation', 'RandomSolarize', 'RandomVerticalFlip', 'Resize', 'TenCrop', 'ToPILImage', 'ToTensor', 'TrivialAugmentWide', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_pil_constants', '_presets', 'autoaugment', 'functional', 'functional_pil', 'functional_tensor', 'transforms']\n"
     ]
    }
   ],
   "source": [
    "print(   dir(transforms)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30cd7557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x1F7764A63D0>\n"
     ]
    }
   ],
   "source": [
    "print(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "297cacc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "\n",
    "print(img_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ee615",
   "metadata": {},
   "source": [
    "The transforms can be passed directly to tnhe entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a0a2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc67577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t , _ = tensor_cifar10[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c1647d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2431, 0.1961, 0.1804,  ..., 0.6549, 0.7176, 0.5373],\n",
      "         [0.2471, 0.2157, 0.2039,  ..., 0.6392, 0.6706, 0.5686],\n",
      "         [0.2275, 0.2510, 0.2196,  ..., 0.6000, 0.5882, 0.4824],\n",
      "         ...,\n",
      "         [0.6745, 0.5608, 0.5098,  ..., 0.3686, 0.5529, 0.5451],\n",
      "         [0.7176, 0.5882, 0.3137,  ..., 0.3176, 0.5294, 0.5608],\n",
      "         [0.8196, 0.7137, 0.5451,  ..., 0.2314, 0.5098, 0.6627]],\n",
      "\n",
      "        [[0.2510, 0.1961, 0.1725,  ..., 0.6745, 0.7216, 0.5333],\n",
      "         [0.2549, 0.2078, 0.1961,  ..., 0.6627, 0.6824, 0.5725],\n",
      "         [0.2431, 0.2588, 0.2353,  ..., 0.6078, 0.6039, 0.5020],\n",
      "         ...,\n",
      "         [0.5294, 0.4314, 0.2196,  ..., 0.2941, 0.4235, 0.4118],\n",
      "         [0.5725, 0.4627, 0.2510,  ..., 0.2824, 0.4627, 0.4902],\n",
      "         [0.6824, 0.5922, 0.4275,  ..., 0.2118, 0.4667, 0.6118]],\n",
      "\n",
      "        [[0.1725, 0.1020, 0.0745,  ..., 0.2706, 0.2980, 0.2824],\n",
      "         [0.1451, 0.1020, 0.1059,  ..., 0.2392, 0.2941, 0.3020],\n",
      "         [0.1412, 0.1451, 0.1451,  ..., 0.2431, 0.2510, 0.2235],\n",
      "         ...,\n",
      "         [0.3882, 0.3294, 0.1647,  ..., 0.2196, 0.3373, 0.3176],\n",
      "         [0.4588, 0.3725, 0.1725,  ..., 0.2353, 0.3843, 0.4314],\n",
      "         [0.5647, 0.4824, 0.3255,  ..., 0.1843, 0.4353, 0.6275]]])\n"
     ]
    }
   ],
   "source": [
    "print(img_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e3281",
   "metadata": {},
   "source": [
    "## Normalize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f291ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgs_list = [ img_t  for img_t, _ in tensor_cifar10 ]\n",
    "\n",
    "imgs = torch.stack( imgs_list, dim=3 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abec28ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32, 50000])\n"
     ]
    }
   ],
   "source": [
    "print(   imgs.shape   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac3226fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 51200000])\n"
     ]
    }
   ],
   "source": [
    "view1 = imgs.view(3, -1)\n",
    "print(view1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "149cafef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4914, 0.4822, 0.4465])\n"
     ]
    }
   ],
   "source": [
    "view1 = view1.mean(dim=1)\n",
    "print(view1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d08b1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "view2 = imgs.view(3, -1).std(dim=1)\n",
    "print(view2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "105da5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transformed_cifar10 = datasets.CIFAR10(data_path, train=True, download= False,\n",
    "                                      transform = transforms.Compose([\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(view1, view2)\n",
    "                                      ]))\n",
    "\n",
    "\n",
    "transformed_cifar10_val = datasets.CIFAR10(data_path, train=False, download= False,\n",
    "                                      transform = transforms.Compose([\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(view1, view2)\n",
    "                                      ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f9721",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "632ec9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0:0, 2:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c754ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'bird']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a02ba4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar2 = [  (img, label_map[label])  for img, label in transformed_cifar10 if label in [0, 2]    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71b277e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar2_val = [  (img, label_map[label])  for img, label in transformed_cifar10_val if label in [0, 2]    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b316a0a",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1aa17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63a8ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(   [1.0, 2.0, 3.0 ]   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8714580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(   softmax(x).sum()   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50111b78",
   "metadata": {},
   "source": [
    "\n",
    "## Architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2da75326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_out = 2 \n",
    "\n",
    "model_mlp = nn.Sequential(\n",
    "          nn.Linear(3072, 512),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(512, n_out),\n",
    "          nn.Softmax(dim=1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c07193b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_2DL = nn.Sequential(\n",
    "     nn.Linear(3072, 1024),\n",
    "     nn.ReLU(),                  ## nn.GeLU()\n",
    "     nn.Linear(1024, 512),\n",
    "     nn.ReLU(),\n",
    "     nn.Linear(512, 128),\n",
    "     nn.ReLU(),\n",
    "     nn.Linear(128, 2),\n",
    "     nn.LogSoftmax(dim=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "923d09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## negative log likelihood\n",
    "\n",
    "## loss_fn = nn.NLLLoss()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd78dd",
   "metadata": {},
   "source": [
    "\n",
    "## DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6565a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6f17af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## model_fn = model_mlp\n",
    "model_fn = model_2DL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa28ee",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ea2dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001  ## 1e-2    ## 0.001\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "## optimizer = optim.SGD(  model_fn.parameters(), lr=learning_rate )\n",
    "optimizer = optim.Adam(  model_fn.parameters(), lr=learning_rate )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a84636e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3791, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3750, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5521, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3310, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1398, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8638, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2040, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3896, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4379, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1516, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1693, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0872, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0264, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0921, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1013, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0620, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0816, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0024, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1911, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2181, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1774, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0684, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1925, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3211, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0306, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2562, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0263, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0160, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0574, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0121, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0387, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0128, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1915, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0560, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0075, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0347, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0025, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0181, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0368, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0146, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0439, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3434, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0022, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0100, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0112, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2220, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0075, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0096, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0271, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1985e-05, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0048, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0077, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0825, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0294, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0027, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0028, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0065, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0043, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0059, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0054, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.7802e-05, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0085, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0120, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0654, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0078, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0054, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0063, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0043, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0084, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0037, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0142, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0237, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0714, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0301, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0262, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0285, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0048, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.7078e-05, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0057, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0489e-06, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0016, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3310e-06, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0115, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:       ## imgs [64x3x32x32]\n",
    "        ## print(imgs.shape)\n",
    "        ## resize for network\n",
    "        batch_size = imgs.shape[0]\n",
    "        imgs_resized = imgs.view(batch_size, -1)    ## imgs_resized [64, 3072]\n",
    "        ## print(imgs_resized.shape)\n",
    "        outputs = model_fn(imgs_resized)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db55338",
   "metadata": {},
   "source": [
    "## After training, now testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33fcd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loader = torch.utils.data.DataLoader(   cifar2_val, batch_size=2000, shuffle=False  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2fa1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_metric = 0\n",
    "total   = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0001984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs    = model_fn(    imgs.view(batch_size, -1)     )\n",
    "        _, pred = torch.max(  outputs, dim=1  )\n",
    "        \n",
    "        temp_metric =  temp_metric +  int( (pred == labels).sum()  )\n",
    "        total       = total + batch_size\n",
    "\n",
    "print(   temp_metric/total    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e52f3a",
   "metadata": {},
   "source": [
    "## All performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f733cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(cifar2_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ccb9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loader = torch.utils.data.DataLoader(   cifar2_val, batch_size=2000, shuffle=False  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3afe1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_stats_percentage_train_test(algorithm_name, y_test, y_pred):    \n",
    "     print(\"------------------------------------------------------\")\n",
    "     print(\"------------------------------------------------------\")\n",
    "    \n",
    "     print(\"algorithm is: \", algorithm_name)\n",
    "        \n",
    "     print('Accuracy: %.2f' % accuracy_score(y_test,   y_pred) )\n",
    "     \n",
    "     confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "     print(\"confusion matrix\")\n",
    "     print(confmat)\n",
    "     print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "     print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "     print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "535379c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "algorithm is:  2DL\n",
      "Accuracy: 0.84\n",
      "confusion matrix\n",
      "[[879 121]\n",
      " [190 810]]\n",
      "Precision: 0.846\n",
      "Recall: 0.845\n",
      "F1-measure: 0.844\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs    = model_fn(    imgs.view(batch_size, -1)     )\n",
    "        _, preds = torch.max(  outputs, dim=1  )\n",
    "        print_stats_percentage_train_test(\"2DL\", labels, preds)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

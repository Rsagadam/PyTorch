{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d531447",
   "metadata": {},
   "source": [
    "## CIFAR 10\n",
    "\n",
    "classification (10 classes) \n",
    "\n",
    "CIFAR 10 consists of: 60,000 tiny 32 x 32 color RGB images\n",
    "\n",
    "labeled with integer 1 to 10 classes \n",
    "\n",
    "airplane (0), car (1), etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ec8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "## import imageio\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba26161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/cifar10data/'\n",
    "\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395a6942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf1d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f9f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data/cifar10data/\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "print(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "840a7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = cifar10[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26722904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d31b3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f03c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AugMix', 'AutoAugment', 'AutoAugmentPolicy', 'CenterCrop', 'ColorJitter', 'Compose', 'ConvertImageDtype', 'FiveCrop', 'GaussianBlur', 'Grayscale', 'InterpolationMode', 'Lambda', 'LinearTransformation', 'Normalize', 'PILToTensor', 'Pad', 'RandAugment', 'RandomAdjustSharpness', 'RandomAffine', 'RandomApply', 'RandomAutocontrast', 'RandomChoice', 'RandomCrop', 'RandomEqualize', 'RandomErasing', 'RandomGrayscale', 'RandomHorizontalFlip', 'RandomInvert', 'RandomOrder', 'RandomPerspective', 'RandomPosterize', 'RandomResizedCrop', 'RandomRotation', 'RandomSolarize', 'RandomVerticalFlip', 'Resize', 'TenCrop', 'ToPILImage', 'ToTensor', 'TrivialAugmentWide', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_pil_constants', '_presets', 'autoaugment', 'functional', 'functional_pil', 'functional_tensor', 'transforms']\n"
     ]
    }
   ],
   "source": [
    "print(   dir(transforms)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e025809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x21E8B3CFA60>\n"
     ]
    }
   ],
   "source": [
    "print(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64230298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "\n",
    "print(img_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73aeaf",
   "metadata": {},
   "source": [
    "The transforms can be passed directly to tnhe entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48963fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00866504",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t , _ = tensor_cifar10[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "691171ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2431, 0.1961, 0.1804,  ..., 0.6549, 0.7176, 0.5373],\n",
      "         [0.2471, 0.2157, 0.2039,  ..., 0.6392, 0.6706, 0.5686],\n",
      "         [0.2275, 0.2510, 0.2196,  ..., 0.6000, 0.5882, 0.4824],\n",
      "         ...,\n",
      "         [0.6745, 0.5608, 0.5098,  ..., 0.3686, 0.5529, 0.5451],\n",
      "         [0.7176, 0.5882, 0.3137,  ..., 0.3176, 0.5294, 0.5608],\n",
      "         [0.8196, 0.7137, 0.5451,  ..., 0.2314, 0.5098, 0.6627]],\n",
      "\n",
      "        [[0.2510, 0.1961, 0.1725,  ..., 0.6745, 0.7216, 0.5333],\n",
      "         [0.2549, 0.2078, 0.1961,  ..., 0.6627, 0.6824, 0.5725],\n",
      "         [0.2431, 0.2588, 0.2353,  ..., 0.6078, 0.6039, 0.5020],\n",
      "         ...,\n",
      "         [0.5294, 0.4314, 0.2196,  ..., 0.2941, 0.4235, 0.4118],\n",
      "         [0.5725, 0.4627, 0.2510,  ..., 0.2824, 0.4627, 0.4902],\n",
      "         [0.6824, 0.5922, 0.4275,  ..., 0.2118, 0.4667, 0.6118]],\n",
      "\n",
      "        [[0.1725, 0.1020, 0.0745,  ..., 0.2706, 0.2980, 0.2824],\n",
      "         [0.1451, 0.1020, 0.1059,  ..., 0.2392, 0.2941, 0.3020],\n",
      "         [0.1412, 0.1451, 0.1451,  ..., 0.2431, 0.2510, 0.2235],\n",
      "         ...,\n",
      "         [0.3882, 0.3294, 0.1647,  ..., 0.2196, 0.3373, 0.3176],\n",
      "         [0.4588, 0.3725, 0.1725,  ..., 0.2353, 0.3843, 0.4314],\n",
      "         [0.5647, 0.4824, 0.3255,  ..., 0.1843, 0.4353, 0.6275]]])\n"
     ]
    }
   ],
   "source": [
    "print(img_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813f6f7",
   "metadata": {},
   "source": [
    "## Normalize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40647808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgs_list = [ img_t  for img_t, _ in tensor_cifar10 ]\n",
    "\n",
    "imgs = torch.stack( imgs_list, dim=3 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8a90071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32, 50000])\n"
     ]
    }
   ],
   "source": [
    "print(   imgs.shape   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b29ced13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 51200000])\n"
     ]
    }
   ],
   "source": [
    "view1 = imgs.view(3, -1)\n",
    "print(view1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7af55d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4914, 0.4822, 0.4465])\n"
     ]
    }
   ],
   "source": [
    "view1 = view1.mean(dim=1)\n",
    "print(view1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0f2cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "view2 = imgs.view(3, -1).std(dim=1)\n",
    "print(view2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a150223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transformed_cifar10 = datasets.CIFAR10(data_path, train=True, download= False,\n",
    "                                      transform = transforms.Compose([\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(view1, view2)\n",
    "                                      ]))\n",
    "\n",
    "\n",
    "transformed_cifar10_val = datasets.CIFAR10(data_path, train=False, download= False,\n",
    "                                      transform = transforms.Compose([\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(view1, view2)\n",
    "                                      ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb6d90",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b2a3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0:0, 2:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e97ca519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'bird']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09ce7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar2 = [  (img, label_map[label])  for img, label in transformed_cifar10 if label in [0, 2]    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "caff3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar2_val = [  (img, label_map[label])  for img, label in transformed_cifar10_val if label in [0, 2]    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d96bf6",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7dfc0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edacc72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(   [1.0, 2.0, 3.0 ]   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06948a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(   softmax(x).sum()   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9f7e3",
   "metadata": {},
   "source": [
    "\n",
    "## Convert vectors from 32x32x3   to 1x3072\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08f80a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_out = 2 \n",
    "\n",
    "model_mlp = nn.Sequential(\n",
    "\n",
    "          nn.Linear(3072, 512),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(512, n_out),\n",
    "          nn.Softmax(dim=1)\n",
    "\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30a026a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## negative log likelihood\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15b72b",
   "metadata": {},
   "source": [
    "\n",
    "## DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "282cea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd3ea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e3ddd",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63d657a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-2    ## 0.001\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "optimizer = optim.SGD(  model_fn.parameters(), lr=learning_rate )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52de7110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.6310, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8222, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.6661, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7360, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.5773, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7751, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7511, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7833, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8841, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8429, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7426, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8910, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8685, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.5708, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9189, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7067, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8423, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8313, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8704, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9413, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9840, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7041, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9135, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7682, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.6819, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8078, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9112, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.6887, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8469, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8059, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8331, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9957, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8928, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8821, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8412, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8442, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7086, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7651, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7932, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8668, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8119, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8028, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9822, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8667, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7987, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8360, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7558, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8240, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9746, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8443, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9308, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8313, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8383, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8245, grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.6845, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [59], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m imgs_resized \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)    \u001b[38;5;66;03m## imgs_resized [64, 3072]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m## print(imgs_resized.shape)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_resized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\2022_py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\2022_py38\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\2022_py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\2022_py38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:       ## imgs [64x3x32x32]\n",
    "        ## print(imgs.shape)\n",
    "        ## resize for network\n",
    "        batch_size = imgs.shape[0]\n",
    "        imgs_resized = imgs.view(batch_size, -1)    ## imgs_resized [64, 3072]\n",
    "        ## print(imgs_resized.shape)\n",
    "        outputs = model_fn(imgs_resized)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25d67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29abdac8",
   "metadata": {},
   "source": [
    "\n",
    "## ADA NVIDIA paper\n",
    "\n",
    "* This code was originally developed by NVIDIA. I have modified it for my research project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fe348",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# All augmentations are disabled by default; individual augmentations can\n",
    "# be enabled by setting their probability multipliers to 1.\n",
    "\n",
    "\n",
    "class AugmentPipe(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        xflip=0, rotate90=0, xint=0, xint_max=0.125,\n",
    "        scale=0, rotate=0, aniso=0, xfrac=0, scale_std=0.2, rotate_max=1, aniso_std=0.2, xfrac_std=0.125,\n",
    "        brightness=0, contrast=0, lumaflip=0, hue=0, saturation=0, brightness_std=0.2, contrast_std=0.5, hue_max=1, saturation_std=1,\n",
    "        imgfilter=0, imgfilter_bands=[1,1,1,1], imgfilter_std=1,\n",
    "        noise=0, cutout=0, noise_std=0.1, cutout_size=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_buffer('p', torch.ones([]))       # Overall multiplier for augmentation probability.\n",
    "\n",
    "        # Pixel blitting.\n",
    "        self.xflip            = float(xflip)            # Probability multiplier for x-flip.\n",
    "        self.rotate90         = float(rotate90)         # Probability multiplier for 90 degree rotations.\n",
    "        self.xint             = float(xint)             # Probability multiplier for integer translation.\n",
    "        self.xint_max         = float(xint_max)         # Range of integer translation, relative to image dimensions.\n",
    "\n",
    "        # General geometric transformations.\n",
    "        self.scale            = float(scale)            # Probability multiplier for isotropic scaling.\n",
    "        self.rotate           = float(rotate)           # Probability multiplier for arbitrary rotation.\n",
    "        self.aniso            = float(aniso)            # Probability multiplier for anisotropic scaling.\n",
    "        self.xfrac            = float(xfrac)            # Probability multiplier for fractional translation.\n",
    "        self.scale_std        = float(scale_std)        # Log2 standard deviation of isotropic scaling.\n",
    "        self.rotate_max       = float(rotate_max)       # Range of arbitrary rotation, 1 = full circle.\n",
    "        self.aniso_std        = float(aniso_std)        # Log2 standard deviation of anisotropic scaling.\n",
    "        self.xfrac_std        = float(xfrac_std)        # Standard deviation of frational translation, relative to image dimensions.\n",
    "\n",
    "        # Color transformations.\n",
    "        self.brightness       = float(brightness)       # Probability multiplier for brightness.\n",
    "        self.contrast         = float(contrast)         # Probability multiplier for contrast.\n",
    "        self.lumaflip         = float(lumaflip)         # Probability multiplier for luma flip.\n",
    "        self.hue              = float(hue)              # Probability multiplier for hue rotation.\n",
    "        self.saturation       = float(saturation)       # Probability multiplier for saturation.\n",
    "        self.brightness_std   = float(brightness_std)   # Standard deviation of brightness.\n",
    "        self.contrast_std     = float(contrast_std)     # Log2 standard deviation of contrast.\n",
    "        self.hue_max          = float(hue_max)          # Range of hue rotation, 1 = full circle.\n",
    "        self.saturation_std   = float(saturation_std)   # Log2 standard deviation of saturation.\n",
    "\n",
    "        # Image-space filtering.\n",
    "        self.imgfilter        = float(imgfilter)        # Probability multiplier for image-space filtering.\n",
    "        self.imgfilter_bands  = list(imgfilter_bands)   # Probability multipliers for individual frequency bands.\n",
    "        self.imgfilter_std    = float(imgfilter_std)    # Log2 standard deviation of image-space filter amplification.\n",
    "\n",
    "        # Image-space corruptions.\n",
    "        self.noise            = float(noise)            # Probability multiplier for additive RGB noise.\n",
    "        self.cutout           = float(cutout)           # Probability multiplier for cutout.\n",
    "        self.noise_std        = float(noise_std)        # Standard deviation of additive RGB noise.\n",
    "        self.cutout_size      = float(cutout_size)      # Size of the cutout rectangle, relative to image dimensions.\n",
    "\n",
    "        # Setup orthogonal lowpass filter for geometric augmentations.\n",
    "        self.register_buffer('Hz_geom', upfirdn2d.setup_filter(wavelets['sym6']))\n",
    "\n",
    "        # Construct filter bank for image-space filtering.\n",
    "        Hz_lo = np.asarray(wavelets['sym2'])            # H(z)\n",
    "        Hz_hi = Hz_lo * ((-1) ** np.arange(Hz_lo.size)) # H(-z)\n",
    "        Hz_lo2 = np.convolve(Hz_lo, Hz_lo[::-1]) / 2    # H(z) * H(z^-1) / 2\n",
    "        Hz_hi2 = np.convolve(Hz_hi, Hz_hi[::-1]) / 2    # H(-z) * H(-z^-1) / 2\n",
    "        Hz_fbank = np.eye(4, 1)                         # Bandpass(H(z), b_i)\n",
    "        for i in range(1, Hz_fbank.shape[0]):\n",
    "            Hz_fbank = np.dstack([Hz_fbank, np.zeros_like(Hz_fbank)]).reshape(Hz_fbank.shape[0], -1)[:, :-1]\n",
    "            Hz_fbank = scipy.signal.convolve(Hz_fbank, [Hz_lo2])\n",
    "            Hz_fbank[i, (Hz_fbank.shape[1] - Hz_hi2.size) // 2 : (Hz_fbank.shape[1] + Hz_hi2.size) // 2] += Hz_hi2\n",
    "        self.register_buffer('Hz_fbank', torch.as_tensor(Hz_fbank, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, images, debug_percentile=None):\n",
    "        assert isinstance(images, torch.Tensor) and images.ndim == 4\n",
    "        batch_size, num_channels, height, width = images.shape\n",
    "        device = images.device\n",
    "        if debug_percentile is not None:\n",
    "            debug_percentile = torch.as_tensor(debug_percentile, dtype=torch.float32, device=device)\n",
    "\n",
    "        # -------------------------------------\n",
    "        # Select parameters for pixel blitting.\n",
    "        # -------------------------------------\n",
    "\n",
    "        # Initialize inverse homogeneous 2D transform: G_inv @ pixel_out ==> pixel_in\n",
    "        I_3 = torch.eye(3, device=device)\n",
    "        G_inv = I_3\n",
    "\n",
    "        # Apply x-flip with probability (xflip * strength).\n",
    "        if self.xflip > 0:\n",
    "            i = torch.floor(torch.rand([batch_size], device=device) * 2)\n",
    "            i = torch.where(torch.rand([batch_size], device=device) < self.xflip * self.p, i, torch.zeros_like(i))\n",
    "            if debug_percentile is not None:\n",
    "                i = torch.full_like(i, torch.floor(debug_percentile * 2))\n",
    "            G_inv = G_inv @ scale2d_inv(1 - 2 * i, 1)\n",
    "\n",
    "        # Apply 90 degree rotations with probability (rotate90 * strength).\n",
    "        if self.rotate90 > 0:\n",
    "            i = torch.floor(torch.rand([batch_size], device=device) * 4)\n",
    "            i = torch.where(torch.rand([batch_size], device=device) < self.rotate90 * self.p, i, torch.zeros_like(i))\n",
    "            if debug_percentile is not None:\n",
    "                i = torch.full_like(i, torch.floor(debug_percentile * 4))\n",
    "            G_inv = G_inv @ rotate2d_inv(-np.pi / 2 * i)\n",
    "\n",
    "        # Apply integer translation with probability (xint * strength).\n",
    "        if self.xint > 0:\n",
    "            t = (torch.rand([batch_size, 2], device=device) * 2 - 1) * self.xint_max\n",
    "            t = torch.where(torch.rand([batch_size, 1], device=device) < self.xint * self.p, t, torch.zeros_like(t))\n",
    "            if debug_percentile is not None:\n",
    "                t = torch.full_like(t, (debug_percentile * 2 - 1) * self.xint_max)\n",
    "            G_inv = G_inv @ translate2d_inv(torch.round(t[:,0] * width), torch.round(t[:,1] * height))\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # Select parameters for general geometric transformations.\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        # Apply isotropic scaling with probability (scale * strength).\n",
    "        if self.scale > 0:\n",
    "            s = torch.exp2(torch.randn([batch_size], device=device) * self.scale_std)\n",
    "            s = torch.where(torch.rand([batch_size], device=device) < self.scale * self.p, s, torch.ones_like(s))\n",
    "            if debug_percentile is not None:\n",
    "                s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.scale_std))\n",
    "            G_inv = G_inv @ scale2d_inv(s, s)\n",
    "\n",
    "        # Apply pre-rotation with probability p_rot.\n",
    "        p_rot = 1 - torch.sqrt((1 - self.rotate * self.p).clamp(0, 1)) # P(pre OR post) = p\n",
    "        if self.rotate > 0:\n",
    "            theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.rotate_max\n",
    "            theta = torch.where(torch.rand([batch_size], device=device) < p_rot, theta, torch.zeros_like(theta))\n",
    "            if debug_percentile is not None:\n",
    "                theta = torch.full_like(theta, (debug_percentile * 2 - 1) * np.pi * self.rotate_max)\n",
    "            G_inv = G_inv @ rotate2d_inv(-theta) # Before anisotropic scaling.\n",
    "\n",
    "        # Apply anisotropic scaling with probability (aniso * strength).\n",
    "        if self.aniso > 0:\n",
    "            s = torch.exp2(torch.randn([batch_size], device=device) * self.aniso_std)\n",
    "            s = torch.where(torch.rand([batch_size], device=device) < self.aniso * self.p, s, torch.ones_like(s))\n",
    "            if debug_percentile is not None:\n",
    "                s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.aniso_std))\n",
    "            G_inv = G_inv @ scale2d_inv(s, 1 / s)\n",
    "\n",
    "        # Apply post-rotation with probability p_rot.\n",
    "        if self.rotate > 0:\n",
    "            theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.rotate_max\n",
    "            theta = torch.where(torch.rand([batch_size], device=device) < p_rot, theta, torch.zeros_like(theta))\n",
    "            if debug_percentile is not None:\n",
    "                theta = torch.zeros_like(theta)\n",
    "            G_inv = G_inv @ rotate2d_inv(-theta) # After anisotropic scaling.\n",
    "\n",
    "        # Apply fractional translation with probability (xfrac * strength).\n",
    "        if self.xfrac > 0:\n",
    "            t = torch.randn([batch_size, 2], device=device) * self.xfrac_std\n",
    "            t = torch.where(torch.rand([batch_size, 1], device=device) < self.xfrac * self.p, t, torch.zeros_like(t))\n",
    "            if debug_percentile is not None:\n",
    "                t = torch.full_like(t, torch.erfinv(debug_percentile * 2 - 1) * self.xfrac_std)\n",
    "            G_inv = G_inv @ translate2d_inv(t[:,0] * width, t[:,1] * height)\n",
    "\n",
    "        # ----------------------------------\n",
    "        # Execute geometric transformations.\n",
    "        # ----------------------------------\n",
    "\n",
    "        # Execute if the transform is not identity.\n",
    "        if G_inv is not I_3:\n",
    "\n",
    "            # Calculate padding.\n",
    "            cx = (width - 1) / 2\n",
    "            cy = (height - 1) / 2\n",
    "            cp = matrix([-cx, -cy, 1], [cx, -cy, 1], [cx, cy, 1], [-cx, cy, 1], device=device) # [idx, xyz]\n",
    "            cp = G_inv @ cp.t() # [batch, xyz, idx]\n",
    "            Hz_pad = self.Hz_geom.shape[0] // 4\n",
    "            margin = cp[:, :2, :].permute(1, 0, 2).flatten(1) # [xy, batch * idx]\n",
    "            margin = torch.cat([-margin, margin]).max(dim=1).values # [x0, y0, x1, y1]\n",
    "            margin = margin + misc.constant([Hz_pad * 2 - cx, Hz_pad * 2 - cy] * 2, device=device)\n",
    "            margin = margin.max(misc.constant([0, 0] * 2, device=device))\n",
    "            margin = margin.min(misc.constant([width-1, height-1] * 2, device=device))\n",
    "            mx0, my0, mx1, my1 = margin.ceil().to(torch.int32)\n",
    "\n",
    "            # Pad image and adjust origin.\n",
    "            images = torch.nn.functional.pad(input=images, pad=[mx0,mx1,my0,my1], mode='reflect')\n",
    "            G_inv = translate2d((mx0 - mx1) / 2, (my0 - my1) / 2) @ G_inv\n",
    "\n",
    "            # Upsample.\n",
    "            images = upfirdn2d.upsample2d(x=images, f=self.Hz_geom, up=2)\n",
    "            G_inv = scale2d(2, 2, device=device) @ G_inv @ scale2d_inv(2, 2, device=device)\n",
    "            G_inv = translate2d(-0.5, -0.5, device=device) @ G_inv @ translate2d_inv(-0.5, -0.5, device=device)\n",
    "\n",
    "            # Execute transformation.\n",
    "            shape = [batch_size, num_channels, (height + Hz_pad * 2) * 2, (width + Hz_pad * 2) * 2]\n",
    "            G_inv = scale2d(2 / images.shape[3], 2 / images.shape[2], device=device) @ G_inv @ scale2d_inv(2 / shape[3], 2 / shape[2], device=device)\n",
    "            grid = torch.nn.functional.affine_grid(theta=G_inv[:,:2,:], size=shape, align_corners=False)\n",
    "            images = grid_sample_gradfix.grid_sample(images, grid)\n",
    "\n",
    "            # Downsample and crop.\n",
    "            images = upfirdn2d.downsample2d(x=images, f=self.Hz_geom, down=2, padding=-Hz_pad*2, flip_filter=True)\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # Select parameters for color transformations.\n",
    "        # --------------------------------------------\n",
    "\n",
    "        # Initialize homogeneous 3D transformation matrix: C @ color_in ==> color_out\n",
    "        I_4 = torch.eye(4, device=device)\n",
    "        C = I_4\n",
    "\n",
    "        # Apply brightness with probability (brightness * strength).\n",
    "        if self.brightness > 0:\n",
    "            b = torch.randn([batch_size], device=device) * self.brightness_std\n",
    "            b = torch.where(torch.rand([batch_size], device=device) < self.brightness * self.p, b, torch.zeros_like(b))\n",
    "            if debug_percentile is not None:\n",
    "                b = torch.full_like(b, torch.erfinv(debug_percentile * 2 - 1) * self.brightness_std)\n",
    "            C = translate3d(b, b, b) @ C\n",
    "\n",
    "        # Apply contrast with probability (contrast * strength).\n",
    "        if self.contrast > 0:\n",
    "            c = torch.exp2(torch.randn([batch_size], device=device) * self.contrast_std)\n",
    "            c = torch.where(torch.rand([batch_size], device=device) < self.contrast * self.p, c, torch.ones_like(c))\n",
    "            if debug_percentile is not None:\n",
    "                c = torch.full_like(c, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.contrast_std))\n",
    "            C = scale3d(c, c, c) @ C\n",
    "\n",
    "        # Apply luma flip with probability (lumaflip * strength).\n",
    "        v = misc.constant(np.asarray([1, 1, 1, 0]) / np.sqrt(3), device=device) # Luma axis.\n",
    "        if self.lumaflip > 0:\n",
    "            i = torch.floor(torch.rand([batch_size, 1, 1], device=device) * 2)\n",
    "            i = torch.where(torch.rand([batch_size, 1, 1], device=device) < self.lumaflip * self.p, i, torch.zeros_like(i))\n",
    "            if debug_percentile is not None:\n",
    "                i = torch.full_like(i, torch.floor(debug_percentile * 2))\n",
    "            C = (I_4 - 2 * v.ger(v) * i) @ C # Householder reflection.\n",
    "\n",
    "        # Apply hue rotation with probability (hue * strength).\n",
    "        if self.hue > 0 and num_channels > 1:\n",
    "            theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.hue_max\n",
    "            theta = torch.where(torch.rand([batch_size], device=device) < self.hue * self.p, theta, torch.zeros_like(theta))\n",
    "            if debug_percentile is not None:\n",
    "                theta = torch.full_like(theta, (debug_percentile * 2 - 1) * np.pi * self.hue_max)\n",
    "            C = rotate3d(v, theta) @ C # Rotate around v.\n",
    "\n",
    "        # Apply saturation with probability (saturation * strength).\n",
    "        if self.saturation > 0 and num_channels > 1:\n",
    "            s = torch.exp2(torch.randn([batch_size, 1, 1], device=device) * self.saturation_std)\n",
    "            s = torch.where(torch.rand([batch_size, 1, 1], device=device) < self.saturation * self.p, s, torch.ones_like(s))\n",
    "            if debug_percentile is not None:\n",
    "                s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.saturation_std))\n",
    "            C = (v.ger(v) + (I_4 - v.ger(v)) * s) @ C\n",
    "\n",
    "        # ------------------------------\n",
    "        # Execute color transformations.\n",
    "        # ------------------------------\n",
    "\n",
    "        # Execute if the transform is not identity.\n",
    "        if C is not I_4:\n",
    "            images = images.reshape([batch_size, num_channels, height * width])\n",
    "            if num_channels == 3:\n",
    "                images = C[:, :3, :3] @ images + C[:, :3, 3:]\n",
    "            elif num_channels == 1:\n",
    "                C = C[:, :3, :].mean(dim=1, keepdims=True)\n",
    "                images = images * C[:, :, :3].sum(dim=2, keepdims=True) + C[:, :, 3:]\n",
    "            else:\n",
    "                raise ValueError('Image must be RGB (3 channels) or L (1 channel)')\n",
    "            images = images.reshape([batch_size, num_channels, height, width])\n",
    "\n",
    "        # ----------------------\n",
    "        # Image-space filtering.\n",
    "        # ----------------------\n",
    "\n",
    "        if self.imgfilter > 0:\n",
    "            num_bands = self.Hz_fbank.shape[0]\n",
    "            assert len(self.imgfilter_bands) == num_bands\n",
    "            expected_power = misc.constant(np.array([10, 1, 1, 1]) / 13, device=device) # Expected power spectrum (1/f).\n",
    "\n",
    "            # Apply amplification for each band with probability (imgfilter * strength * band_strength).\n",
    "            g = torch.ones([batch_size, num_bands], device=device) # Global gain vector (identity).\n",
    "            for i, band_strength in enumerate(self.imgfilter_bands):\n",
    "                t_i = torch.exp2(torch.randn([batch_size], device=device) * self.imgfilter_std)\n",
    "                t_i = torch.where(torch.rand([batch_size], device=device) < self.imgfilter * self.p * band_strength, t_i, torch.ones_like(t_i))\n",
    "                if debug_percentile is not None:\n",
    "                    t_i = torch.full_like(t_i, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.imgfilter_std)) if band_strength > 0 else torch.ones_like(t_i)\n",
    "                t = torch.ones([batch_size, num_bands], device=device)                  # Temporary gain vector.\n",
    "                t[:, i] = t_i                                                           # Replace i'th element.\n",
    "                t = t / (expected_power * t.square()).sum(dim=-1, keepdims=True).sqrt() # Normalize power.\n",
    "                g = g * t                                                               # Accumulate into global gain.\n",
    "\n",
    "            # Construct combined amplification filter.\n",
    "            Hz_prime = g @ self.Hz_fbank                                    # [batch, tap]\n",
    "            Hz_prime = Hz_prime.unsqueeze(1).repeat([1, num_channels, 1])   # [batch, channels, tap]\n",
    "            Hz_prime = Hz_prime.reshape([batch_size * num_channels, 1, -1]) # [batch * channels, 1, tap]\n",
    "\n",
    "            # Apply filter.\n",
    "            p = self.Hz_fbank.shape[1] // 2\n",
    "            images = images.reshape([1, batch_size * num_channels, height, width])\n",
    "            images = torch.nn.functional.pad(input=images, pad=[p,p,p,p], mode='reflect')\n",
    "            images = conv2d_gradfix.conv2d(input=images, weight=Hz_prime.unsqueeze(2), groups=batch_size*num_channels)\n",
    "            images = conv2d_gradfix.conv2d(input=images, weight=Hz_prime.unsqueeze(3), groups=batch_size*num_channels)\n",
    "            images = images.reshape([batch_size, num_channels, height, width])\n",
    "\n",
    "        # ------------------------\n",
    "        # Image-space corruptions.\n",
    "        # ------------------------\n",
    "\n",
    "        # Apply additive RGB noise with probability (noise * strength).\n",
    "        if self.noise > 0:\n",
    "            sigma = torch.randn([batch_size, 1, 1, 1], device=device).abs() * self.noise_std\n",
    "            sigma = torch.where(torch.rand([batch_size, 1, 1, 1], device=device) < self.noise * self.p, sigma, torch.zeros_like(sigma))\n",
    "            if debug_percentile is not None:\n",
    "                sigma = torch.full_like(sigma, torch.erfinv(debug_percentile) * self.noise_std)\n",
    "            images = images + torch.randn([batch_size, num_channels, height, width], device=device) * sigma\n",
    "\n",
    "        # Apply cutout with probability (cutout * strength).\n",
    "        if self.cutout > 0:\n",
    "            size = torch.full([batch_size, 2, 1, 1, 1], self.cutout_size, device=device)\n",
    "            size = torch.where(torch.rand([batch_size, 1, 1, 1, 1], device=device) < self.cutout * self.p, size, torch.zeros_like(size))\n",
    "            center = torch.rand([batch_size, 2, 1, 1, 1], device=device)\n",
    "            if debug_percentile is not None:\n",
    "                size = torch.full_like(size, self.cutout_size)\n",
    "                center = torch.full_like(center, debug_percentile)\n",
    "            coord_x = torch.arange(width, device=device).reshape([1, 1, 1, -1])\n",
    "            coord_y = torch.arange(height, device=device).reshape([1, 1, -1, 1])\n",
    "            mask_x = (((coord_x + 0.5) / width - center[:, 0]).abs() >= size[:, 0] / 2)\n",
    "            mask_y = (((coord_y + 0.5) / height - center[:, 1]).abs() >= size[:, 1] / 2)\n",
    "            mask = torch.logical_or(mask_x, mask_y).to(torch.float32)\n",
    "            images = images * mask\n",
    "\n",
    "        return images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metrics                 = [],       # Metrics to evaluate during training.\n",
    "random_seed             = 0,        # Global random seed.\n",
    "rank                    = 0,        # Rank of the current process in [0, num_gpus[.\n",
    "batch_size              = 4,        # Total batch size for one training iteration. Can be larger than batch_gpu * num_gpus.\n",
    "batch_gpu               = 4,        # Number of samples processed at a time by one GPU.\n",
    "ema_kimg                = 10,       # Half-life of the exponential moving average (EMA) of generator weights.\n",
    "ema_rampup              = None,     # EMA ramp-up coefficient.\n",
    "G_reg_interval          = 4,        # How often to perform regularization for G? None = disable lazy regularization.\n",
    "D_reg_interval          = 16,       # How often to perform regularization for D? None = disable lazy regularization.\n",
    "augment_p               = 0,        # Initial value of augmentation probability.\n",
    "ada_target              = None,     # ADA target value. None = fixed p.\n",
    "ada_interval            = 4,        # How often to perform ADA adjustment?\n",
    "ada_kimg                = 500,      # ADA adjustment speed, measured in how many kimg it takes for p to increase/decrease by one unit.\n",
    "total_kimg              = 25000,    # Total length of the training, measured in thousands of real images.\n",
    "kimg_per_tick           = 4       # Progress snapshot interval.\n",
    "image_snapshot_ticks    = 50       # How often to save image snapshots? None = disable.\n",
    "network_snapshot_ticks  = 50       # How often to save network snapshots? None = disable.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2fdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728042a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
